#-------------------------------------------------------------
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
# 
#   http://www.apache.org/licenses/LICENSE-2.0
# 
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
#-------------------------------------------------------------

# Implements multiclass C-SVM with squared slack variables, uses one-against-the-rest binary classifiers
# 

# 100k Dataset:
# hadoop jar SystemDS.jar -f MultiClassSVM.dml -args itau/svm/X_100k_500 itau/svm/y_100k 0 2 0.001 1.0 100 itau/svm/w_100k_1

# 5M Dataset:
# hadoop jar SystemDS.jar -f MultiClassSVM.dml -args itau/svm/X_5m_5k itau/svm/y_5m 0 2 0.001 1.0 100 itau/svm/w_100k_1

# Assume SVM_HOME is set to the home of the dml script
# Assume input and output directories are on hdfs as INPUT_DIR and OUTPUT_DIR
# Assume number of classes is 10, epsilon = 0.000001, lambda=1.0, max_iterations = 100
# hadoop jar SystemDS.jar -f $SVM_HOME/MultiClassSVM.dml -args $INPUT_DIR/X $INPUT_DIR/y intercept 10 0.000001 1.0 100 $OUTPUT_DIR/w

X = read($1)
check_X = sum(X)
if(check_X == 0){
	print("X has no non-zeros")
}else{
Y = read($2)
intercept = $3
num_classes = $4
epsilon = $5
lambda = $6
max_iterations = $7
 
num_samples = nrow(X)
num_features = ncol(X)

if (intercept == 1) {
 ones  = Rand(rows=num_samples, cols=1, min=1, max=1, pdf="uniform");
 X = cbind(X, ones);
}

iter_class = 1

Y_local = 2 * (Y == iter_class) - 1
w_class = Rand(rows=num_features, cols=1, min=0, max=0, pdf="uniform")
if (intercept == 1) {
 zero_matrix = Rand(rows=1, cols=1, min=0.0, max=0.0);
 w_class = t(cbind(t(w_class), zero_matrix));
}

g_old = t(X) %*% Y_local
s = g_old
iter = 0
continue = 1
while(continue == 1)  {
  # minimizing primal obj along direction s
  step_sz = 0
  Xd = X %*% s
  wd = lambda * sum(w_class * s)
  dd = lambda * sum(s * s)
  continue1 = 1
  while(continue1 == 1){
   tmp_w = w_class + step_sz*s
   out = 1 - Y_local * (X %*% tmp_w)
   sv = (out > 0)
   out = out * sv
   g = wd + step_sz*dd - sum(out * Y_local * Xd)
   h = dd + sum(Xd * sv * Xd)
   step_sz = step_sz - g/h
   if (g*g/h < 0.0000000001){
    continue1 = 0
   }
  }
 
  #update weights
  w_class = w_class + step_sz*s
 
  out = 1 - Y_local * (X %*% w_class)
  sv = (out > 0)
  out = sv * out
  obj = 0.5 * sum(out * out) + lambda/2 * sum(w_class * w_class)
  g_new = t(X) %*% (out * Y_local) - lambda * w_class

  tmp = sum(s * g_old)
  
  train_acc = sum((Y_local*(X%*%w_class)) >= 0)/num_samples*100
  print("For class " + iter_class + " iteration " + iter + " training accuracy: " + train_acc)
   
  if((step_sz*tmp < epsilon*obj) | (iter >= max_iterations-1)){
   continue = 0
  }
 
  #non-linear CG step
  be = sum(g_new * g_new)/sum(g_old * g_old)
  s = be * s + g_new
  g_old = g_new

  iter = iter + 1
 }


w = w_class
iter_class = iter_class + 1

while(iter_class <= num_classes){
 Y_local = 2 * (Y == iter_class) - 1
# w_class = Rand(rows=num_features, cols=1, min=0, max=0, pdf="uniform")
 w_class = Rand(rows=ncol(X), cols=1, min=0, max=0, pdf="uniform")
 if (intercept == 1) {
 	zero_matrix = Rand(rows=1, cols=1, min=0.0, max=0.0);
 	w_class = t(cbind(t(w_class), zero_matrix));
 }
 
 g_old = t(X) %*% Y_local
 s = g_old

 iter = 0
 continue = 1
 while(continue == 1)  {
  # minimizing primal obj along direction s
  step_sz = 0
  Xd = X %*% s
  wd = lambda * sum(w_class * s)
  dd = lambda * sum(s * s)
  continue1 = 1
  while(continue1 == 1){
   tmp_w = w_class + step_sz*s
   out = 1 - Y_local * (X %*% tmp_w)
   sv = (out > 0)
   out = out * sv
   g = wd + step_sz*dd - sum(out * Y_local * Xd)
   h = dd + sum(Xd * sv * Xd)
   step_sz = step_sz - g/h
   if (g*g/h < 0.0000000001){
    continue1 = 0
   }
  }
 
  #update weights
  w_class = w_class + step_sz*s
 
  out = 1 - Y_local * (X %*% w_class)
  sv = (out > 0)
  out = sv * out
  obj = 0.5 * sum(out * out) + lambda/2 * sum(w_class * w_class)
  g_new = t(X) %*% (out * Y_local) - lambda * w_class

  tmp = sum(s * g_old)
  
  train_acc = sum(Y_local*(X%*%w_class) >= 0)/num_samples*100
  print("For class " + iter_class + " iteration " + iter + " training accuracy: " + train_acc)
   
  if((step_sz*tmp < epsilon*obj) | (iter >= max_iterations-1)){
   continue = 0
  }
 
  #non-linear CG step
  be = sum(g_new * g_new)/sum(g_old * g_old)
  s = be * s + g_new
  g_old = g_new

  iter = iter + 1
 }

 w = cbind(w, w_class) 
 iter_class = iter_class + 1
}

write(w, $8, format="text")
}
